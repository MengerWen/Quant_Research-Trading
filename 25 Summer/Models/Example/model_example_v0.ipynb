{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "af628601",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.linear_model import Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e450c6f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c9ed8e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "42be6499",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ad89b849",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeSeriesLSTM:\n",
    "    def __init__(self, input_size, hidden_size=64, num_layers=2, output_size=1,\n",
    "                 epochs=10, batch_size=32, lr=0.001, device='cpu'):\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.output_size = output_size\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.lr = lr\n",
    "        self.device = device\n",
    "        self.model = LSTMModel(input_size, hidden_size, num_layers, output_size).to(device)\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr)\n",
    "        self.criterion = nn.MSELoss()\n",
    "\n",
    "    def _prepare_data(self, X, y=None):\n",
    "        X_tensor = torch.tensor(X.values.reshape(-1, 1, self.input_size), dtype=torch.float32).to(self.device)\n",
    "        if y is not None:\n",
    "            y_tensor = torch.tensor(y.values.reshape(-1, self.output_size), dtype=torch.float32).to(self.device)\n",
    "            return TensorDataset(X_tensor, y_tensor)\n",
    "        return TensorDataset(X_tensor)\n",
    "\n",
    "    def fit(self, X_train, y_train):\n",
    "        train_dataset = self._prepare_data(X_train, y_train)\n",
    "        train_loader = DataLoader(train_dataset, batch_size=self.batch_size, shuffle=True)\n",
    "\n",
    "        self.model.train()\n",
    "        for epoch in range(self.epochs):\n",
    "            for batch_X, batch_y in train_loader:\n",
    "                self.optimizer.zero_grad()\n",
    "                outputs = self.model(batch_X)\n",
    "                loss = self.criterion(outputs, batch_y)\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "\n",
    "    def predict(self, X_test):\n",
    "        test_dataset = self._prepare_data(X_test)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=self.batch_size, shuffle=False)\n",
    "\n",
    "        self.model.eval()\n",
    "        predictions = []\n",
    "        with torch.no_grad():\n",
    "            for batch_X in test_loader:\n",
    "                outputs = self.model(batch_X[0])\n",
    "                predictions.append(outputs.cpu().numpy())\n",
    "        return np.vstack(predictions)\n",
    "\n",
    "    def score(self, X, y):\n",
    "        y_pred = self.predict(X)\n",
    "        y_true = y.values.reshape(-1, self.output_size)\n",
    "\n",
    "        mse = np.mean((y_true - y_pred)**2)\n",
    "        return -mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "38130b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(X,y,model,n_split = 5,normalize=False):\n",
    "    scores = []\n",
    "    tscv = TimeSeriesSplit(n_splits=n_split)\n",
    "    for fold, (train_index, test_index) in enumerate(tscv.split(X)):\n",
    "        logging.info(f\"--- Fold {fold+1} ---\")\n",
    "\n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "        if normalize:\n",
    "            scaler = MinMaxScaler()\n",
    "            X_train = pd.DataFrame(scaler.fit_transform(X_train), columns=X_train.columns, index=X_train.index)\n",
    "            X_test = pd.DataFrame(scaler.transform(X_test), columns=X_test.columns, index=X_test.index)\n",
    "\n",
    "        logging.info(f\"训练集大小: {len(X_train)}, 测试集大小: {len(X_test)}\")\n",
    "        logging.info(f\"训练集索引范围: {X.index[train_index.min()]}-{X.index[train_index.max()]}\")\n",
    "        logging.info(f\"测试集索引范围: {X.index[test_index.min()]}-{X.index[test_index.max()]}\")\n",
    "\n",
    "        current_model = copy.deepcopy(model)\n",
    "\n",
    "        current_model.fit(X_train, y_train)\n",
    "        predictions = current_model.predict(X_test)\n",
    "        is_score = current_model.score(X_train, y_train)\n",
    "        os_score = current_model.score(X_test, y_test)\n",
    "        scores.append(os_score)\n",
    "\n",
    "        logging.info(f\"训练集评分: {is_score:.4f}, 测试集评分: {os_score:.4f}\")\n",
    "    return -sum(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cd366713",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3213637/3407595317.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data['target'] = data['close'].shift(-10)/data['close'] - 1\n",
      "2025-07-18 10:57:55,096 - INFO - Using device: cpu\n",
      "2025-07-18 10:57:55,880 - INFO - \n",
      "--- Training Ridge Model ---\n",
      "2025-07-18 10:57:55,881 - INFO - --- Fold 1 ---\n",
      "2025-07-18 10:57:55,894 - INFO - 训练集大小: 19967, 测试集大小: 19964\n",
      "2025-07-18 10:57:55,894 - INFO - 训练集索引范围: 2021-10-01 00:00:00-2022-04-26 23:30:00\n",
      "2025-07-18 10:57:55,895 - INFO - 测试集索引范围: 2022-04-26 23:45:00-2022-11-21 03:45:00\n",
      "2025-07-18 10:57:55,956 - INFO - 训练集评分: 0.0029, 测试集评分: -0.0956\n",
      "2025-07-18 10:57:55,957 - INFO - --- Fold 2 ---\n",
      "2025-07-18 10:57:55,967 - INFO - 训练集大小: 39931, 测试集大小: 19964\n",
      "2025-07-18 10:57:55,968 - INFO - 训练集索引范围: 2021-10-01 00:00:00-2022-11-21 03:45:00\n",
      "2025-07-18 10:57:55,968 - INFO - 测试集索引范围: 2022-11-21 04:00:00-2023-06-17 02:45:00\n",
      "2025-07-18 10:57:55,980 - INFO - 训练集评分: 0.0033, 测试集评分: -0.0098\n",
      "2025-07-18 10:57:55,985 - INFO - --- Fold 3 ---\n",
      "2025-07-18 10:57:55,994 - INFO - 训练集大小: 59895, 测试集大小: 19964\n",
      "2025-07-18 10:57:55,996 - INFO - 训练集索引范围: 2021-10-01 00:00:00-2023-06-17 02:45:00\n",
      "2025-07-18 10:57:55,997 - INFO - 测试集索引范围: 2023-06-17 03:00:00-2024-01-11 01:45:00\n",
      "2025-07-18 10:57:56,015 - INFO - 训练集评分: 0.0024, 测试集评分: 0.0006\n",
      "2025-07-18 10:57:56,016 - INFO - --- Fold 4 ---\n",
      "2025-07-18 10:57:56,028 - INFO - 训练集大小: 79859, 测试集大小: 19964\n",
      "2025-07-18 10:57:56,029 - INFO - 训练集索引范围: 2021-10-01 00:00:00-2024-01-11 01:45:00\n",
      "2025-07-18 10:57:56,030 - INFO - 测试集索引范围: 2024-01-11 02:00:00-2024-08-06 00:45:00\n",
      "2025-07-18 10:57:56,049 - INFO - 训练集评分: 0.0027, 测试集评分: 0.0016\n",
      "2025-07-18 10:57:56,050 - INFO - --- Fold 5 ---\n",
      "2025-07-18 10:57:56,065 - INFO - 训练集大小: 99823, 测试集大小: 19964\n",
      "2025-07-18 10:57:56,065 - INFO - 训练集索引范围: 2021-10-01 00:00:00-2024-08-06 00:45:00\n",
      "2025-07-18 10:57:56,066 - INFO - 测试集索引范围: 2024-08-06 01:00:00-2025-03-01 23:45:00\n",
      "2025-07-18 10:57:56,090 - INFO - 训练集评分: 0.0027, 测试集评分: -0.0010\n",
      "2025-07-18 10:57:56,091 - INFO - Ridge Model Total  Score: 0.1041\n",
      "2025-07-18 10:57:56,092 - INFO - \n",
      "--- Training LightGBM Model ---\n",
      "2025-07-18 10:57:56,092 - INFO - --- Fold 1 ---\n",
      "2025-07-18 10:57:56,095 - INFO - 训练集大小: 19967, 测试集大小: 19964\n",
      "2025-07-18 10:57:56,095 - INFO - 训练集索引范围: 2021-10-01 00:00:00-2022-04-26 23:30:00\n",
      "2025-07-18 10:57:56,096 - INFO - 测试集索引范围: 2022-04-26 23:45:00-2022-11-21 03:45:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.074359 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3825\n",
      "[LightGBM] [Info] Number of data points in the train set: 19967, number of used features: 15\n",
      "[LightGBM] [Info] Start training from score -0.000010\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-18 10:59:31,968 - INFO - 训练集评分: 0.1918, 测试集评分: -0.0768\n",
      "2025-07-18 10:59:31,973 - INFO - --- Fold 2 ---\n",
      "2025-07-18 10:59:31,976 - INFO - 训练集大小: 39931, 测试集大小: 19964\n",
      "2025-07-18 10:59:31,977 - INFO - 训练集索引范围: 2021-10-01 00:00:00-2022-11-21 03:45:00\n",
      "2025-07-18 10:59:31,978 - INFO - 测试集索引范围: 2022-11-21 04:00:00-2023-06-17 02:45:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.059053 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3825\n",
      "[LightGBM] [Info] Number of data points in the train set: 39931, number of used features: 15\n",
      "[LightGBM] [Info] Start training from score -0.000186\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-18 11:01:34,648 - INFO - 训练集评分: 0.1275, 测试集评分: -0.0318\n",
      "2025-07-18 11:01:34,651 - INFO - --- Fold 3 ---\n",
      "2025-07-18 11:01:34,656 - INFO - 训练集大小: 59895, 测试集大小: 19964\n",
      "2025-07-18 11:01:34,656 - INFO - 训练集索引范围: 2021-10-01 00:00:00-2023-06-17 02:45:00\n",
      "2025-07-18 11:01:34,657 - INFO - 测试集索引范围: 2023-06-17 03:00:00-2024-01-11 01:45:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.064339 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3825\n",
      "[LightGBM] [Info] Number of data points in the train set: 59895, number of used features: 15\n",
      "[LightGBM] [Info] Start training from score -0.000034\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-18 11:03:38,634 - INFO - 训练集评分: 0.1019, 测试集评分: -0.0149\n",
      "2025-07-18 11:03:38,639 - INFO - --- Fold 4 ---\n",
      "2025-07-18 11:03:38,650 - INFO - 训练集大小: 79859, 测试集大小: 19964\n",
      "2025-07-18 11:03:38,654 - INFO - 训练集索引范围: 2021-10-01 00:00:00-2024-01-11 01:45:00\n",
      "2025-07-18 11:03:38,655 - INFO - 测试集索引范围: 2024-01-11 02:00:00-2024-08-06 00:45:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.067966 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3825\n",
      "[LightGBM] [Info] Number of data points in the train set: 79859, number of used features: 15\n",
      "[LightGBM] [Info] Start training from score 0.000051\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-18 11:06:11,340 - INFO - 训练集评分: 0.0882, 测试集评分: -0.0090\n",
      "2025-07-18 11:06:11,341 - INFO - --- Fold 5 ---\n",
      "2025-07-18 11:06:11,348 - INFO - 训练集大小: 99823, 测试集大小: 19964\n",
      "2025-07-18 11:06:11,349 - INFO - 训练集索引范围: 2021-10-01 00:00:00-2024-08-06 00:45:00\n",
      "2025-07-18 11:06:11,349 - INFO - 测试集索引范围: 2024-08-06 01:00:00-2025-03-01 23:45:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.065072 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3825\n",
      "[LightGBM] [Info] Number of data points in the train set: 99823, number of used features: 15\n",
      "[LightGBM] [Info] Start training from score 0.000067\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-18 11:08:24,947 - INFO - 训练集评分: 0.0770, 测试集评分: -0.0126\n",
      "2025-07-18 11:08:24,948 - INFO - LightGBM Model Total  Score: 0.1451\n",
      "2025-07-18 11:08:24,949 - INFO - \n",
      "--- Training LSTM Model ---\n",
      "2025-07-18 11:08:24,949 - INFO - --- Fold 1 ---\n",
      "2025-07-18 11:08:24,956 - INFO - 训练集大小: 19967, 测试集大小: 19964\n",
      "2025-07-18 11:08:24,957 - INFO - 训练集索引范围: 2021-10-01 00:00:00-2022-04-26 23:30:00\n",
      "2025-07-18 11:08:24,958 - INFO - 测试集索引范围: 2022-04-26 23:45:00-2022-11-21 03:45:00\n",
      "2025-07-18 11:41:32,952 - INFO - 训练集评分: -0.0001, 测试集评分: -0.0001\n",
      "2025-07-18 11:41:32,953 - INFO - --- Fold 2 ---\n",
      "2025-07-18 11:41:32,964 - INFO - 训练集大小: 39931, 测试集大小: 19964\n",
      "2025-07-18 11:41:32,965 - INFO - 训练集索引范围: 2021-10-01 00:00:00-2022-11-21 03:45:00\n",
      "2025-07-18 11:41:32,966 - INFO - 测试集索引范围: 2022-11-21 04:00:00-2023-06-17 02:45:00\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 43\u001b[39m\n\u001b[32m     40\u001b[39m logging.info(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mLightGBM Model Total  Score: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlgb_score_sum\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     42\u001b[39m logging.info(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m--- Training LSTM Model ---\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m43\u001b[39m lstm_score_sum = \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlstm_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnormalize\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     44\u001b[39m logging.info(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mLSTM Model Total  Score: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlstm_score_sum\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 21\u001b[39m, in \u001b[36mtrain\u001b[39m\u001b[34m(X, y, model, n_split, normalize)\u001b[39m\n\u001b[32m     17\u001b[39m logging.info(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m测试集索引范围: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mX.index[test_index.min()]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mX.index[test_index.max()]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     19\u001b[39m current_model = copy.deepcopy(model)\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m \u001b[43mcurrent_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     22\u001b[39m predictions = current_model.predict(X_test)\n\u001b[32m     23\u001b[39m is_score = current_model.score(X_train, y_train)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 33\u001b[39m, in \u001b[36mTimeSeriesLSTM.fit\u001b[39m\u001b[34m(self, X_train, y_train)\u001b[39m\n\u001b[32m     31\u001b[39m outputs = \u001b[38;5;28mself\u001b[39m.model(batch_X)\n\u001b[32m     32\u001b[39m loss = \u001b[38;5;28mself\u001b[39m.criterion(outputs, batch_y)\n\u001b[32m---> \u001b[39m\u001b[32m33\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     34\u001b[39m \u001b[38;5;28mself\u001b[39m.optimizer.step()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/cta_v1/lib/python3.11/site-packages/torch/_tensor.py:648\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    638\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    639\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    640\u001b[39m         Tensor.backward,\n\u001b[32m    641\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    646\u001b[39m         inputs=inputs,\n\u001b[32m    647\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m648\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    650\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/cta_v1/lib/python3.11/site-packages/torch/autograd/__init__.py:353\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    348\u001b[39m     retain_graph = create_graph\n\u001b[32m    350\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m353\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/cta_v1/lib/python3.11/site-packages/torch/autograd/graph.py:824\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    822\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    823\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m824\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    825\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    826\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    827\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    828\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "if __name__=='__main__':\n",
    "    factor_data_path = '/public/data/factor_data'\n",
    "    file_name = 'BTCUSDT_15m_2020_2025_factor_data.pkl'\n",
    "\n",
    "    data = pd.read_pickle(os.path.join(factor_data_path,file_name))\n",
    "\n",
    "    data['target'] = data['close'].shift(-10)/data['close'] - 1\n",
    "\n",
    "    begin = '2021-10-01'\n",
    "    split = '2025-03-01'\n",
    "    selected_factors = [f'c_chu0{i}' for i in range(37,52)]\n",
    "    workding_data = data[selected_factors+['target']][begin:split].dropna()\n",
    "\n",
    "    X_data = workding_data[selected_factors]\n",
    "    y_data = workding_data['target']\n",
    "\n",
    "    ridge_model = Ridge(0.0008)\n",
    "    lgb_model = lgb.LGBMRegressor(random_state=42)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    logging.info(f\"Using device: {device}\")\n",
    "\n",
    "\n",
    "    lstm_model = TimeSeriesLSTM(\n",
    "        input_size=len(selected_factors),\n",
    "        hidden_size=64,\n",
    "        num_layers=2,\n",
    "        epochs=5,\n",
    "        batch_size=32,\n",
    "        lr=0.001,\n",
    "        device=device\n",
    "    )\n",
    "\n",
    "    logging.info(\"\\n--- Training Ridge Model ---\")\n",
    "    ridge_score_sum = train(X_data, y_data, ridge_model, normalize=True)\n",
    "    logging.info(f\"Ridge Model Total  Score: {ridge_score_sum:.4f}\")\n",
    "\n",
    "    logging.info(\"\\n--- Training LightGBM Model ---\")\n",
    "    lgb_score_sum = train(X_data, y_data, lgb_model, normalize=False)\n",
    "    logging.info(f\"LightGBM Model Total  Score: {lgb_score_sum:.4f}\")\n",
    "\n",
    "    logging.info(\"\\n--- Training LSTM Model ---\")\n",
    "    lstm_score_sum = train(X_data, y_data, lstm_model, normalize=True)\n",
    "    logging.info(f\"LSTM Model Total  Score: {lstm_score_sum:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cta_v1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
